\chapter{Various Error Criteria} \label{chap:relerror}
The error criterion in \cref{prob:findzerocont} requires that the location of one zero of the function be identified within an interval of half-width $\varepsilon$.  The true solution of $f(x) = 0$ can be written as the set $f^{-1}(0)$.  If the zero-finding algorithm outputs $x_0$, then the true error of the output is $\min_{x \in f^{-1}(0)} \abs{x-x_0}$. The absolute error criterion in \cref{prob:findzerocont} that the output, $x_0$, must satisfy can be expressed as 
\begin{equation} \label{eq:fzeroerrorcrit}
    \crit\bigl(x_0,f^{-1}(0),\varepsilon\bigr) = 
    \begin{cases} \true, & \exists x \in f^{-1}(0) \text{ such that } \abs{x-x_0} \le \varepsilon,\\
    \false, & \text{otherwise}.
    \end{cases}
\end{equation}

In practice, it is not possible to know the true error of the output, but \cref{alg:zeroBisectionAB} iteratively refines the interval $[x_{\text{left}}, x_{\text{right}}]$ that must contain at least one zero of $f$. 
The data-driven stopping criterion for \cref{alg:zeroBisectionAB} may be expressed by replacing the true solution by the all possible partial solutions as
\begin{equation}
    \datacrit\bigl(x_0,[x_{\text{left}},x_{\text{right}}],\varepsilon\bigr) = 
    \begin{cases} \true, & \crit\bigl(x_0,\{x\},\varepsilon\bigr) = \true \ \ \forall x \in [x_{\text{left}},x_{\text{right}}] ,\\
    \false, & \text{otherwise},
    \end{cases}
\end{equation}

Choosing $x_0 = (x_{\text{left}}+x_{\text{right}})/2$ as is done in \cref{alg:zeroBisectionAB} gives the stopping criterion the best chance of being $\true$.  When $\datacrit\bigl(x_0,[x_{\text{left}},x_{\text{right}}],\varepsilon\bigr) = \true$, then \cref{alg:zeroBisectionAB} stops and returns $x_0$.

In some cases, the practitioner may wish to choose a different kind of error criterion than an absolute one, such as a relative error criterion or one involving a combination of absolute and relative error tolerances.  In such cases, choosing the middle of the interval containing the solution may be not be best.  In this chapter we explore various error criteria, their corresponding stopping criteria, and the choices of outputs.

\section{A Generic Numerical Problem and Template Algorithm}

Let's generalize the zero-finding situation to a generic numerical problem.  As before, we let $\cf$ denote the set of input functions, which must be large enough so that the successful algorithm is widely applicable but constrained so that a successful algorithm exists. 

\begin{NumProblem}[Generic Numerical Problem]
\label{prob:generalProblem}
\problemspecs{set of functions $\cf$ \\ 
solution operator $\sol: \cf \to \cg$}
{black-box function $f \in \cf$ \\ error tolerance (vector) $\veps$}
{$\out \in \ca$ such that \\ \qquad $\crit(\out,\sol(f),\veps) = \true$}
\end{NumProblem}

The solution operator, $\sol$, maps an element in $\cf$ to a \emph{set} of acceptable solutions, not a single solution. This is to allow for the case of non-unique acceptable solutions. For example, if $f: x \mapsto x^3 - x$ defined on $[-2,2]$, then $\sol(f) = \{-1, 0, 1\}$ for zero-finding \cref{prob:findzerocont}, where only one zero is required, but  $\sol(f) = \{ \{-1, 0, 1\} \}$ for \cref{prob:findzerocontall}, where all zeros are required.  The set of sets of acceptable solutions is denoted $\cg$.

The output required from the problem comes from the set of acceptable solutions $\ca$.  The elements of $\ca$ are the elements of the elements of $\cg$, i.e., $\ca = \{S \in G : G \in \cg\}$.  For $f: x \mapsto x^3 - x$ defined on $[-2,2]$ in zero-finding \cref{prob:findzerocont}, $\out$ is a number sufficiently close to $-1$ or $0$ or $1$.  For \cref{prob:findzerocontall}, $\out$ for this same function is a set sufficiently close to  $\{-1, 0, 1\}$.  The sets $\cf$, $\cg$, and $\ca$ are tabulated for different problems in \cref{tab:genericproblem}.

The error criterion for the problem, $\crit: \ca \times \cg \times (0,\infty)^s \to \{\true, \false\}$, specifies how close the output must be to an element in $\sol(f)$.  Absolute error criteria take the form
\begin{equation} \label{eq:generalerrorcrit}
    \crit\bigl(\out,\sol(f),\varepsilon\bigr) = 
    \begin{cases} \true, & \exists G \in \sol(f) \text{ such that } \vdist(\out,G) \le \veps,\\
    \false, & \text{otherwise},
    \end{cases}
\end{equation}
where $\vdist(\out,G)$ is an $s$-dimensional vector of distances, which must be no greater than $\veps$ element-wise for $\crit$ to be true. The error criterion defined in \eqref{eq:fzeroerrorcrit} this is of this form, but other error criteria are possible and common.


\begin{table}[H]
    \centering
    \caption{Examples of specific problems following Generic \cref{prob:generalProblem}}
      {\small \begin{tabular}{c>{\centering}m{0.23\textwidth}>{\centering}m{0.13\textwidth}>{\centering}m{0.25\textwidth}>{\centering}m{0.2\textwidth}}
       \multicolumn{2}{c}{Problem} & Inputs, $\cf$ & $\cg$ & $\ca$
         \tabularnewline \toprule
         \ref{prob:findzerocont} & 
         $\sol(f) = f^{-1}(0)$ & $\bigcup_{a < b} C[a,b]$ & closed, bounded subsets of $\reals$ & $\reals$
        \tabularnewline \midrule
        \ref{prob:findzerocontall} &
        $\sol(f) = \{f^{-1}(0)\}$ & $\bigcup_{a < b} C[a,b]$ & sets with a single element that is a closed, bounded subset of $\reals$  & 
        closed, bounded subsets of $\reals$
        \tabularnewline \bottomrule
        \end{tabular} }
    \label{tab:genericproblem}
\end{table}

Next we define a template \cref{alg:template} for \cref{prob:generalProblem}.  The algorithm works with $\cs$, a set of acceptable solutions consistent with the function data.  We initialize $\cs$ to be $\ca$, the whole set of acceptable solutions, which is also are set of possible outputs.
At each iteration, the algorithm samples $f$ further and shrinks $\cs$ to be consistent with all function data.  A data-based stopping criterion is defined by determining whether the error criterion for the problem is true for all possible acceptable solutions in $\cs$:
\begin{equation}
    \datacrit(\out,\cs,\veps) = \begin{cases} \true, & \crit(\out,S,\veps) = \true \quad \forall S \in \cs, \\ \false, & \text{otherwise} .
    \end{cases}
\end{equation}
When $\cs$ becomes small enough so that an output exists that $\out$ exists that makes the stopping criterion $\true$, then the algorithm terminates.

\begin{algorithm}[H]
\caption{Template algorithm for Generic Numerical \cref{prob:generalProblem} \label{alg:template}}
	\begin{algorithmic}
   \State Initialize $\cs$, the set containing acceptable solutions consistent with function data, to be $\ca$
   \State Shrink $\cs$ based on an initial sample of $f$
    \Repeat 
    \State Shrink $\cs$ further based on further sampling of $f$
    \State Choose $\out$ optimally
    \Until $\datacrit(\out,\cs,\varepsilon) = \true$
    \RETURN $\out$
    \end{algorithmic}
\end{algorithm}



\section{Absolute Error Criterion} \label{sec:abserror}


of the general case defined in the first row of \cref{tab:errorcrit}.
\begin{table}[H]
    \centering
    \caption{Conditions under which the error criterion is true and the corresponding optimal output}
    \begin{equation*}
       \begin{array}{cc}
         \crit(\out,\sol(f),\veps)
         & \out\\ \toprule
        \vdist(\out,G) \le \veps & 
        \argmin \sup_{\sol \in \cs} \dist(\cdot,\sol)} \\
         \midrule
         \dist(\out,\sol) \le \varepsilon \sol 
         & \err(\out,\cs) \le \varepsilon
        \end{array} 
    \end{equation*}
    
    \label{tab:errorcrit}
\end{table}

\section{Relative Error}

Testing