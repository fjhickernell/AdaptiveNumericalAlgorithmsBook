\chapter{Various Error Criteria} \label{chap:relerror}
The error criterion in \cref{prob:findzerocont} requires that the location of one zero of the function be identified within an interval of half-width $\varepsilon$.  The true solution of $f(x) = 0$ can be written as the set $f^{-1}(0)$.  If the zero-finding algorithm outputs $x_0$, then the true error of the output is $\min_{x \in f^{-1}(0)} \abs{x-x_0}$. The absolute error criterion in \cref{prob:findzerocont} that the output, $x_0$, must satisfy can be expressed as 
\begin{equation} \label{eq:fzeroerrorcrit}
    \crit\bigl(x_0,f^{-1}(0),\varepsilon\bigr) = 
    \begin{cases} \true, & \exists x \in f^{-1}(0) \text{ such that } \abs{x-x_0} \le \varepsilon,\\
    \false, & \text{otherwise}.
    \end{cases}
\end{equation}

In practice, it is not possible to know the true error of the output, but \cref{alg:zeroBisectionAB} iteratively refines the interval $[x_{\text{left}}, x_{\text{right}}]$ that must contain at least one zero of $f$. 
The data-driven stopping criterion for \cref{alg:zeroBisectionAB} may be expressed by replacing the true solution by the all possible partial solutions as
\begin{equation}
    \datacrit\bigl(x_0,[x_{\text{left}},x_{\text{right}}],\varepsilon\bigr) = 
    \begin{cases} \true, & \crit\bigl(x_0,\{x\},\varepsilon\bigr) = \true \ \ \forall x \in [x_{\text{left}},x_{\text{right}}] ,\\
    \false, & \text{otherwise},
    \end{cases}
\end{equation}

Choosing $x_0 = (x_{\text{left}}+x_{\text{right}})/2$ as is done in \cref{alg:zeroBisectionAB} gives the stopping criterion the best chance of being $\true$.  When $\datacrit\bigl(x_0,[x_{\text{left}},x_{\text{right}}],\varepsilon\bigr) = \true$, then \cref{alg:zeroBisectionAB} stops and returns $x_0$.

In some cases, the practitioner may wish to choose a different kind of error criterion than an absolute one, such as a relative error criterion or one involving a combination of absolute and relative error tolerances.  In such cases, choosing the middle of the interval containing the solution may be not be best.  In this chapter we explore various error criteria, their corresponding stopping criteria, and the choices of outputs.

\section{A Generic Numerical Problem and Template Algorithm}

Let's generalize the zero-finding situation to a generic numerical problem.  As before, we let $\cf$ denote the set of input functions, which must be large enough so that the successful algorithm is widely applicable but constrained so that a successful algorithm exists. 

\begin{NumProblem}[Generic Numerical Problem]
\label{prob:generalProblem}
\problemspecs{set of functions $\cf$ \\ 
solution operator $\sol: \cf \to \cg$}
{black-box function $f \in \cf$ \\ error tolerance (vector) $\veps$}
{$\out \in \ca$ such that \\ \qquad $\crit(\out,\sol(f),\veps) = \true$}
\end{NumProblem}

The solution operator, $\sol$, maps an element in $\cf$ to a \emph{set} of acceptable solutions, not a single solution. This is to allow for the case of non-unique acceptable solutions. For example, if $f: x \mapsto x^3 - x$ defined on $[-2,2]$, then $\sol(f) = \{-1, 0, 1\}$ for zero-finding \cref{prob:findzerocont}, where only one zero is required, but  $\sol(f) = \{ \{-1, 0, 1\} \}$ for \cref{prob:findzerocontall}, where all zeros are required.  The set of sets of acceptable solutions is denoted $\cg$.

The output required from the problem comes from the set of acceptable solutions $\ca$.  The elements of $\ca$ are the elements of the elements of $\cg$, i.e., $\ca = \{S \in G : G \in \cg\}$.  For $f: x \mapsto x^3 - x$ defined on $[-2,2]$ in zero-finding \cref{prob:findzerocont}, $\out$ is a number sufficiently close to $-1$ or $0$ or $1$.  For \cref{prob:findzerocontall}, $\out$ for this same function is a set sufficiently close to  $\{-1, 0, 1\}$.  The sets $\cf$, $\cg$, and $\ca$ are tabulated for different problems in \cref{tab:genericproblem}.  

\begin{table}[H]
    \centering
    \caption{Examples of specific problems following Generic \cref{prob:generalProblem}}
      {\small \begin{tabular}{c>{\centering}m{0.21\textwidth}>{\centering}m{0.15\textwidth}>{\centering}m{0.25\textwidth}>{\centering}m{0.2\textwidth}}
       \multicolumn{2}{c}{Problem} & Inputs, $\cf$ & $\cg$ & $\ca$
         \tabularnewline \toprule
         \ref{prob:findzerocont} & 
         $\sol(f) = f^{-1}(0)$ & $\bigcup_{a < b} C[a,b]$ \\ $f(a)f(b) \le 0$ & closed, bounded subsets of $\reals$ & $\reals$
        \tabularnewline \midrule
        \ref{prob:findzerocontall} &
        $\sol(f) = \{f^{-1}(0)\}$ & $\bigcup_{a < b} C[a,b]$ & sets with a single element: a closed, bounded subset of $\reals$  & 
        closed, bounded subsets of $\reals$
        \tabularnewline \bottomrule
        \end{tabular} }
    \label{tab:genericproblem}
\end{table}

The error criterion, $\crit: \ca \times \cg \times (0,\infty)^s \to \{\true, \false\}$, specifies how close the output must be to an element in $\sol(f)$.  The first input is an algorithm output, the second input is the true solution, and the third input is a (vector) error tolerance.  The error criterion for the zero-finding problem defined in \eqref{eq:fzeroerrorcrit} is an example.  Various error criteria are discussed in this chapter.

Next we define a template \cref{alg:template} for \cref{prob:generalProblem}.  The algorithm works with $\cs$, a set of acceptable solutions consistent with the function data.  We initialize $\cs$ to be $\ca$, the whole set of acceptable solutions, which is also are set of possible outputs.
At each iteration, the algorithm samples $f$ further and shrinks $\cs$ to be consistent with all function data.  A data-based stopping criterion is defined by determining whether the error criterion for the problem is true for all possible acceptable solutions in $\cs$:
\begin{equation}
    \datacrit(\out,\cs,\veps) = \begin{cases} \true, & \crit(\out,S,\veps) = \true \quad \forall S \in \cs, \\ \false, & \text{otherwise} .
    \end{cases}
\end{equation}
When $\cs$ becomes small enough so that an output exists that $\out$ exists that makes the stopping criterion $\true$, then the algorithm terminates.

\begin{algorithm}[H]
\caption{Template algorithm for Generic Numerical \cref{prob:generalProblem} \label{alg:template}}
	\begin{algorithmic}
   \State Initialize $\cs$, the set containing acceptable solutions consistent with function data, to be $\ca$
   \State Shrink $\cs$ based on an initial sample of $f$
    \Repeat 
    \State Shrink $\cs$ further based on further sampling of $f$
    \State Choose $\out$ optimally
    \Until $\datacrit(\out,\cs,\varepsilon) = \true$
    \RETURN $\out$
    \end{algorithmic}
\end{algorithm}



\section{Absolute Error Criterion} \label{sec:abserror}
The error criterion for the zero-finding problem defined in \eqref{eq:fzeroerrorcrit}  is an absolute error criterion, which requires that the output be within $\varepsilon$ of the true answer. The following is a more general absolute error criterion where acceptable solutions are real numbers: 
\begin{equation} \label{eq:absnormerrorcrit}
    \crit(\out,\sol(f),\varepsilon) = 
    \begin{cases} \true, & \exists G \in \sol(f) \text{ with } \abs{\out-G} \le \varepsilon,\\
    \false, & \text{otherwise}.
    \end{cases}
\end{equation}
If $\cs$, the set of acceptable solutions consistent with the function data in \cref{alg:template}, is the interval $\cs = [S_{\lo}, S_{\high}]$, then the optimal choice of $\out$ is the center of the interval.

\cref{tab:errorcrit} shows the optimal choices of the output, $\out$ for various error criteria that can be expressed in terms of the absolute value between the output and the solution and where an acceptable true solution is known to be in $[S_{\lo}, S_{\high}]$.  In the following sections we explore relative and compound error criteria.  In these cases, the interval midpoint may not be the optimal choice for $\out$.


\begin{table}[H]
    \centering
    \caption{Conditions under which the error criterion is true and the corresponding optimal output when an acceptable true solution lies in $\cs = [S_{\lo}, S_{\high}]$.}
    \begin{equation*}
       \begin{array}{ccc}
         \crit(\out,\sol(f),\varepsilon) && \text{Sufficient condition for}\\
         \exists G \in \sol(f) \text{ such that}
         & \out 
         & \datacrit(\out,\cs,\varepsilon) = \true \tabularnewline \toprule
        \abs{\out-G} \le \varepsilon & 
        (S_{\lo} + S_{\high})/2 &
        S_{\high} - S_{\lo} < 2\varepsilon \\
         \midrule
         \abs{\out-G} \le \varepsilon \abs{G} 
         & 
        \end{array} 
    \end{equation*}
    
    \label{tab:errorcrit}
\end{table}

\section{Relative Error}
Sometimes the error requirement is relative to the size of the answer, e.g., 
\begin{equation} \label{eq:absnormerrorcrit}
    \crit(\out,\sol(f),\varepsilon) = 
    \begin{cases} \true, & \exists G \in \sol(f) \text{ with } \abs{\out-G} \le \varepsilon \abs{G},\\
    \false, & \text{otherwise}.
    \end{cases}
\end{equation}
Here we assume that the relative error tolerance, $\varepsilon$. is strictly less than one.
Testing

Testing