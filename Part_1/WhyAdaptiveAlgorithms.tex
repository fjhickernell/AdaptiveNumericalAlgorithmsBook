\chapter{Why Adaptive Numerical Algorithms}
Numerical algorithms are crucial for solving mathematical problems that do not have analytic solutions.  Examples include 
\begin{itemize}
    \item finding zeros of functions,
    \item evaluating integrals,
    \item constructing surrogates of functions that are expensive to evaluate,
    \item optimizing functions,
    \item solving linear equations, and
    \item solving (stochastic/partial) differential equations.
\end{itemize}
Algorithms designers aim to construct procedures that provide a correct answer with a limited amount of computational effort.  

These goals of accuracy and efficiency are often at odds.  Adaption allows us balance these goals by inferring from function data what kind of and how much computational effort is required.

\section{Problem Definition}

The success of an algorithm depends on the scope of the problems that it is designed to solve.  To illustrate this, consider the problem of locating the zeros of functions:
\begin{NumProblem}[Find one zero of a univariate\index{functions!univariate} function in $\cf$] \label{prob:findzero}
\problemspecs[0.45]%
{set of functions $\cf$}%
{black-box function $f \in \cf$ \\ error tolerance $\varepsilon > 0$ }%
{$x_0$ such that $f(x) = 0$ \\ 
    \qquad for some $x \in [x_0 - \varepsilon, x_0 + \varepsilon]$}
\end{NumProblem}

The assumptions and the output define the problem and are also used to define the algorithm. A successful algorithm must take any input satisfying the assumptions and return the desired output.   The user should be assured that the output is accurate provided the assumptions are met.  The output will normally not be an exact answer, but an answer satisfying an error tolerance\index{error tolerance}.  In the problem above this is an absolute error tolerance.  Other error criteria are discussed in \cref{chap:relerror}.

By ``black-box''\index{functions!black-box} we mean that our zero finding algorithm has access to function values, but these come at a computational cost\index{computational cost}. The only a priori knowledge about the function is contained in the definition of $\cf$.

\Cref{prob:findzero} is not fully defined until $\cf$ is specified.  If $\cf$ consists of the single function $x \mapsto 29x - 47$, then zero-finding \cref{alg:zero2947} simply needs to return $47/29$, which can be pre-computed.  If $\cf$ consists of all linear functions, then zero-finding \cref{alg:zeroLinear} can evaluate $f$ at two points and interpolate to find the zero exactly\footnote{We typically ignore the complications of finite precision and round-off error\index{round-off error} unless they are catastrophic.}.  The algorithm is computationally cheap but applies to a quite restricted set of functions.  

\begin{algorithm}[H]
\caption{Direct computation for \cref{prob:findzero} with $\cf = \{x \mapsto 29x - 47 \}$ \label{alg:zero2947}}
	\begin{algorithmic}
    \RETURN $x_0 \leftarrow 47/29$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Linear interpolation for \cref{prob:findzero} with $\cf = \{x \mapsto \alpha  + \beta x : \alpha, \beta \in \reals\}$ \label{alg:zeroLinear}}
	\begin{algorithmic}
    \RETURN $x_0 \leftarrow (f(1) - f(0))/f(0)$
    \end{algorithmic}
\end{algorithm}

Even though the user may only apply an algorithm to a handful of input functions, algorithms are typically designed for reasonably large classes of functions so that these algorithms can server a wide variety of users with diverse input functions.  If truly only one or two cases are of interest, then those solutions can be pre-computed, as \cref{alg:zero2947} does for $\cf$ consisting of just one function.

If $\cf$ consists of continuous functions\index{functions!continuous} defined on $[a,b]$ where $f(a)$ and $f(b)$ have opposite signs, then there must exist at least one zero in this interval.  The bisection algorithm succeeds for \cref{prob:findzero} by halving the interval containing the zero until it is small enough.

\begin{algorithm}
\caption{Bisection for \cref{prob:findzero} with $\cf = \{C[a,b] : f(a)f(b) \le 0\}$  \label{alg:zeroBisection}}
	\begin{algorithmic}
    \State Let $x_{\text{left}} \leftarrow a$, $x_{\text{right}} \leftarrow b$, $y_{\text{left}} \leftarrow \sign(f(a))$, $y_{\text{right}} \leftarrow \sign(f(b))$
    \IfThen {$y_{\text{left}} = 0$}{\Return $x_0 \leftarrow x_\text{left}$}
    \IfThen {$y_{\text{right}} = 0$}{\Return $x_0 \leftarrow x_\text{right}$}
    \State Let $\Delta \leftarrow x_{\text{right}}- x_{\text{left}}$
    \Repeat \Comment{$[x_\text{left}, x_{\text{right}}]$ always contains a zero}
    \IfThen {$y_{\text{left}} = y_{\text{right}}$}{\ErrorMsg{interval does not contain zero}}
    \State Let $\Delta \leftarrow \Delta/2$, $x_{\text{new}} \leftarrow x_{\text{left}} + \Delta$, $y_{\text{new}} \leftarrow \sign(f(x_{\text{new}}))$
    \If {$y_{\text{new}} = 0$}
    \RETURN $x_0 \leftarrow x_\text{new}$
    \ElsIf  {$y_{\text{new}} =  y_{\text{left}}$}
    \State $x_{\text{left}} \leftarrow x_{\text{new}}$, $y_{\text{left}} \leftarrow y_{\text{new}}$
    \Else
        \State $x_{\text{right}} \leftarrow x_{\text{new}}$, $y_{\text{right}} \leftarrow y_{\text{new}}$
    \EndIf
    \Until $\Delta < 2\varepsilon$
    \RETURN $x_0 = x_{\text{left}} + \Delta/2$

    \end{algorithmic}
\end{algorithm}

Bisection \cref{alg:zeroBisection} has a few notable features that set it apart the previous two problem/algorithm pairs.  The set of functions for which it succeeds is much broader.   \Cref{alg:zeroBisection} does not provide an exact answer.  Indeed no algorithm can for this choice of $\cf$. But, the error criterion is satisfied. 

We emphasize that algorithms must be matched to the problem definition.  The problem definition specifies our assumptions, and the algorithm is expected to succeed under those assumptions.  If the problem definition is altered, then our algorithm may be invalid.  If the assumptions are violated, then our algorithm may fail.  For \cref{alg:zeroBisection} we can detect some cases in which a $f$ input into the algorithm falls outside $\cf$, such as if the function values at $a$ and $b$ are the same.  However, we cannot always detect whether $f$ falls outside $\cf$.  If $f$ has a jump discontinuity from negative to positive values, then \cref{alg:zeroBisection} may return an $x_0$ near the jump, even if a true zero exists elsewhere.

For simplicity our algorithm listings do not include all the error-checking that a well coded algorithm would have.  For example, in implementing \cref{alg:zeroBisection} one should also include checks that $a< b$, as implied by the definition of $\cf$.

\section{Computational Cost and Computational Complexity}

\Cref{alg:zeroBisection} is adaptive in the selection of points where $f$ is evaluated; different $f$ may result in different choices of $x_{\text{new}}$, which are chosen strategically. The \emph{computational cost} of an algorithm is defined as the number of the arithmetic operations required to produce the output.  Adaptive \cref{alg:zeroBisection} has a computational cost of $\Order\bigl(\log((b-a)/\varepsilon)\bigr)$.  A smaller tolerance demands more arithmetic operations, which translate into more time.

One can imagine a non-adaptive algorithm that evaluates $f$ at  equally spaced $2\varepsilon$ apart on $[a,b]$ and returns $x_0$ as the midpoint of the subinterval where $f$ changes sign.  Such an algorithm would have a computational cost of $\Order\bigl((b-a)/\varepsilon\bigr)$, which is much greater than $\Order\bigl(\log((b-a)/\varepsilon)\bigr)$ as $\varepsilon \to 0$.  Hence, this non-adaptive algorithm is much less efficient.   

The \emph{computational complexity} of a problem is the minimum computational cost over all algorithms that can solve it.  As with computational cost, we are typically satisfied with finding the order, not the exact expression.  We will show in ?? that the computational complexity of \cref{prob:findzero} with $\cf = \{C[a,b] : f(a)f(b) \le 0\}$ is $\Order\bigl(\log((b-a)/\varepsilon)\bigr)$, making \cref{alg:zeroBisection} (asymptotically) optimal.

Since $a$ and $b$ are assumed fixed in \cref{alg:zeroBisection}, the number of function evaluations is always the same for the same tolerance, $\varepsilon$.  We can broaden the problem by making $a$ and $b$ inputs as follows:

\begin{NumProblem}[Finding one zero of a univariate continuous function]%
\label{prob:findzerocont}
\problemspecs[0.45]%
{$\cf = \bigcup_{a < b} C[a,b]$}%
{black-box function $f \in \cf$ \\ 
$a$ and $b$ with $f \in C[a,b]$ \\
\qquad and $f(a) f(b) \le 0$ \\
tolerance $\varepsilon > 0$ }%
{$x_0$ such that $f(x) = 0$ \\ 
    \qquad for some $x \in [x_0 - \varepsilon, x_0 + \varepsilon]$}
\end{NumProblem}

\begin{algorithm}
\caption{Bisection for \cref{prob:findzerocont}  \label{alg:zeroBisectionAB}}
This is the same as \cref{alg:zeroBisection} but with $a$ and $b$ now inputs.
\end{algorithm}

Since this algorithm is the same as the previous one, the computational cost remains the same, i.e., $\Order\bigl(\log((b-a)/\varepsilon)\bigr)$ as $(b-a)/\varepsilon \to \infty$.  Explicitly including the inputs $a$ and $b$ in the computational cost formula is more informative than just assuming them to be constant.  The computational cost may be quite different if the tolerance is specified relative to $b-a$ versus absolutely.

We stress how the computational complexity of a problem and the computational cost required to solve it depend on the problem definition.  \Cref{alg:zero2947,alg:zeroLinear} have a computational cost of $\Order(1)$, however again, they solve very simple problems.  \Cref{prob:findzero,prob:findzerocont} only require one zero to be found.  Finding all zeros of a function is a harder problem that is addressed in ???  If in  $\cf$ \cref{prob:findzerocont} the requirement that $f(a) f(b) \le 0$ is dropped, then the problem may have no solution. 

We may also measure the information cost, or the number of function values required.  The computational cost and the information cost may be of the same order.  However, in some cases, the cost of obtaining one function value may be orders of magnitude greater than an ordinary arithmetic operation in terms of clock time.  This can happen when a function value is the result of an expensive simulation.  In such cases we may distinguish between the information cost and the total computational cost.



\begin{figure}
    \centering
    Dummy figure
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

\begin{table}[]
    \centering
   Dummy table
    \caption{Caption}
    \label{tab:my_label}
\end{table}