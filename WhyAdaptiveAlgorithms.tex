\chapter{Why Adaptive Numerical Algorithms}
Numerical algorithms are crucial for solving mathematical problems that do not have analytic solutions.  Examples include finding zeros, evaluating integrals, or locating optima of functions, solving linear equations, and solving (stochastic/partial) differential equations.  Algorithms designers aim to construct procedures that provide a correct answer with a limited amount of computational effort.  

These goals of accuracy and efficiency are often at odds.  Adaption allows us balance these two goals by inferring from function data how much computational effort is required.

\section{Problem Definition}

Success depends on the scope of problems that the algorithms are designed to solve.  To illustrate this, consider the problem of locating the zeros of a function:
\problem[0.45]{prob:findzero}%
{set of functions $\cf$}%
{black-box function $f \in \cf$ \\ tolerance $\varepsilon > 0$ }%
{$x_0$ such that $f(x) = 0$ \\ 
    \qquad for some $x \in [x_0 - \varepsilon, x_0 + \varepsilon]$}
The assumptions and the output define the problem to be solved and are also used to define the algorithm. A successful algorithm must take any input satisfying the assumptions and return the desired output.   The user should be assured that the output is accurate, provided the assumptions are met.  

By ``black-box'' we mean that our zero finding algorithm has access to function values for any point in the domain, but these come at a computational cost\footnote{By computational cost we mean the number of arithmetic operations required.  Unless noted otherwise, the cost of evaluating the black-box function is assumed to be $\Order(1)$.}. The only a priori knowledge about the function is in the definition of $\cf$.

If $\cf$ consists of the single function $x \mapsto 29x - 47$, then zero-finding Algorithm \ref{alg:zero2947} simply needs to return $47/29$, which can be pre-computed.  If $\cf$ consists of all linear functions, then zero-finding Algorithm \ref{alg:zeroLinear} can evaluate $f$ at two points and interpolate to find the zero exactly\footnote{We typically ignore the complications of finite precision and round-off error unless they are catastrophic.}.  The algorithm is cheap, but applies to a quite narrow set of functions.  

\begin{algorithm}[H]
\caption{Direct computation for \ref{prob:findzero} with $\cf = \{x \mapsto 29x - 47 \}$ \label{alg:zero2947}}
	\begin{algorithmic}
    \RETURN $x_0 \leftarrow 47/29$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Linear interpolation for \ref{prob:findzero} with $\cf = \{x \mapsto \alpha  + \beta x : \alpha, \beta \in \reals\}$ \label{alg:zeroLinear}}
	\begin{algorithmic}
    \RETURN $x_0 \leftarrow (f(1) - f(0))/f(0)$
    \end{algorithmic}
\end{algorithm}

If $\cf$ consists of continuous functions defined on $[a,b]$ where $f(a)$ and $f(b)$ have opposite signs, then there must exist at least one zero in this interval.  The bisection algorithm succeeds for \ref{prob:findzero} by halving the interval containing the zero until it is small enough.

\begin{algorithm}
\caption{Bisection for \ref{prob:findzero} with $\cf = \{C[a,b] : f(a)f(b) \le 0\}$  \label{alg:zeroBisection}}
	\begin{algorithmic}
    \State Let $x_{\text{left}} \leftarrow a$, $x_{\text{right}} \leftarrow b$, $y_{\text{left}} \leftarrow \sign(f(a))$, $y_{\text{right}} \leftarrow \sign(f(b))$
    \IfThen {$y_{\text{left}} = 0$}{\Return $x_0 \leftarrow x_\text{left}$}
    \IfThen {$y_{\text{right}} = 0$}{\Return $x_0 \leftarrow x_\text{right}$}
    \State Let $\Delta \leftarrow x_{\text{right}}- x_{\text{left}}$
    \Repeat
    \IfThen {$y_{\text{left}} = y_{\text{right}}$}{\ErrorMsg{interval does not contain zero}}
    \State Let $\Delta \leftarrow \Delta/2$, $x_{\text{new}} \leftarrow x_{\text{left}} + \Delta$, $y_{\text{new}} \leftarrow \sign(f(x_{\text{new}}))$
    \If {$y_{\text{new}} = 0$}
    \RETURN $x_0 \leftarrow x_\text{new}$
    \ElsIf  {$y_{\text{new}} =  y_{\text{left}}$}
    \State $x_{\text{left}} \leftarrow x_{\text{new}}$, $y_{\text{left}} \leftarrow y_{\text{new}}$
    \Else
        \State $x_{\text{right}} \leftarrow x_{\text{new}}$, $y_{\text{right}} \leftarrow y_{\text{new}}$
    \EndIf
    \Until $\Delta < 2\varepsilon$
    \RETURN $x_0 = x_{\text{left}} + \Delta/2$

    \end{algorithmic}
\end{algorithm}

Bisection Algorithm \ref{alg:zeroBisection} has a few notable features that set it apart the previous two problem/algorithm pairs.  The set of functions for which it succeeds is much broader.  Algorithm \ref{alg:zeroBisection} does not provide an exact answer.  Indeed no algorithm can for this choice of $\cf$. But, the error criterion is satisfied. 

We emphasize that algorithms must be matched to the problem definition.  The problem definition specifies our assumptions, and the algorithm is expected to succeed under those assumptions.  If the problem definition is altered, then our algorithm may be invalid.  If the assumptions are violated, then our algorithm may fail.  For Algorithm \ref{alg:zeroBisection} we can detect some cases in which a $f$ input into the algorithm falls outside $\cf$, such as if the function values at $a$ and $b$ are the same.  However, we cannot always detect whether $f$ falls outside $\cf$.  If $f$ has a jump discontinuity from negative to positive values, then Algorithm \ref{alg:zeroBisection} may converge on the jump, even if a true zero exists elsewhere.

\section{Computational Cost and Computational Complexity}

Algorithm \ref{alg:zeroBisection} is adaptive in the selection of points where $f$ is evaluated; different $f$ may result in different choices of $x_{\text{new}}$, which are chosen strategically. The \emph{computational cost} of an algorithm is defined as the number of the arithmetic operations required to produce the output.  Adaptive Algorithm \ref{alg:zeroBisection} has a computational cost of $\Order\bigl(\log((b-a)/\varepsilon)\bigr)$.  One can imagine a non-adaptive algorithm that evaluates $f$ at  equally spaced $2\varepsilon$ apart on $[a,b]$ and returns $x_0$ as the midpoint of the subinterval where $f$ changes sign.  Such an algorithm would have a computational cost of $\Order\bigl((b-a)/\varepsilon\bigr)$, which is much greater as $\varepsilon \to 0$.   Algorithms \ref{alg:zero2947} and  \ref{alg:zeroLinear} have a computational cost of $\Order(1)$.

The \emph{computational complexity} of a problem is the minimum computational cost over all algorithms that can solve it.  As with computational cost, we are typically satisfied with finding the order, not the exact expression.  We will show in ?? that the computational complexity of \ref{prob:findzero} with $\cf = \{C[a,b] : f(a)f(b) \le 0\}$ is $\Order\bigl(\log((b-a)/\varepsilon)\bigr)$, making Algorithm \ref{alg:zeroBisection} (asymptotically) optimal.

or the arithmetic operations required is a reasonable way to measure the efficiency of an algorithm.  An algorithm that costs $\Order\bigl(\log(\varepsilon^{-1})\bigr)$

\problem[0.45]{prob:findzerocont}%
{$\cf = \bigcup_{a < b} C[a,b]$}%
{black-box function $f \in \cf$ \\ 
$a$ and $b$ with $f \in C[a,b]$ \\
\qquad and $f(a) f(b) \le 0$ \\
tolerance $\varepsilon > 0$ }%
{$x_0$ such that $f(x) = 0$ \\ 
    \qquad for some $x \in [x_0 - \varepsilon, x_0 + \varepsilon]$}


The bisection algorithm succeeds for zero-finding problem \eqref{prob:findzerocont} by starting with an interval that must contain a zero and halving it until the zero is contained in a small enough interval.













all continuous functions  with opposite signs at the endpoints of the domain, $\cf: = \{ f \in C[a,b] : f(a)f(b) \le 0\}$.  The computational cost is $\Order\bigl((b-a) \log(\varepsilon^{-1}) \bigr)$. A smaller tolerance demands more arithmetic operations, which translate into more time.  If $\cf$ is expanded to become $C[a,b]$, then $f \in \cf$ may have no zeros, and so no algorithm exists that can solve \eqref{prob:findzero}.

If there is at least one algorithm that can solve a problem, then we would prefer the most efficient one.  Efficiency can be measured in wall clock time, but this can vary with software and hardware implementations.  The \emph{computational cost} or the arithmetic operations required is a reasonable way to measure the efficiency of an algorithm.  An algorithm that costs $\Order\bigl(\log(\varepsilon^{-1})\bigr)$ operations is more efficient than one that costs $\Order(\varepsilon^{-1})$, where the error tolerance, $\varepsilon$, is tending to zero. Typically we do not count the cost exactly but are satisfied with its order.

The cost of the most efficient algorithm possible is called the \emph{computational complexity} of the problem.  If we can derive a lower bound on the computational complexity and identify an algorithm that achieves this lower bound, then we know the computational complexity.

We may also measure the information cost, or the number of function values required.  These two may be of similar order

